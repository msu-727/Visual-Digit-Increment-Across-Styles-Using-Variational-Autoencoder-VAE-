{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aHWewYS_cQ3Z"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image,ImageOps\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import random\n",
        "import os\n",
        "\n",
        "dataset_path = '~/datasets'\n",
        "\n",
        "cuda = True\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "batch_size = 50\n",
        "x_dim  = 784\n",
        "hidden_dim = 400\n",
        "latent_dim = 200\n",
        "lr = 1e-3\n",
        "epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9jSY_7RReZSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e95687-6e9d-4517-9e25-d2a374a6ec47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /root/datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 52.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/datasets/MNIST/raw/train-images-idx3-ubyte.gz to /root/datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.50MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/datasets/MNIST/raw/train-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /root/datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 12.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.43MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/datasets/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "c = 0.3\n",
        "p = 0.5\n",
        "batch_size = 30\n",
        "class MNISTPairDataset(Dataset):\n",
        "    def __init__(self, c, p, mnist_dataset):\n",
        "        self.mnist_dataset = mnist_dataset\n",
        "        self.c = c\n",
        "        self.p = p\n",
        "        # 创建一个存储每个数字图片索引的字典\n",
        "        self.label_to_indices = {i: [] for i in range(10)}\n",
        "        for idx, (_, label) in enumerate(self.mnist_dataset):\n",
        "            self.label_to_indices[label].append(idx)\n",
        "        self.target_index = {0: 422, 1: 726, 2: 400, 3: 632, 4: 344, 5: 242, 6: 217, 7: 667, 8: 342, 9: 508}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mnist_dataset)\n",
        "    def convert(self,gray_image):\n",
        "      gray_image = gray_image.numpy()\n",
        "      gray_image = gray_image[0]\n",
        "      rgb_image = np.stack([gray_image] * 3, axis=-1)\n",
        "\n",
        "      return rgb_image\n",
        "    def __getitem__(self, idx):\n",
        "        input_img, input_label = self.mnist_dataset[idx]\n",
        "\n",
        "        angle = random.uniform(-15, 15)\n",
        "        input_img = F.rotate(input_img, angle, fill=0)\n",
        "        #random_num = self.c + (1 - self.c) * torch.rand(1)\n",
        "        # random_num = torch.rand(1)\n",
        "        # if random_num < self.p:\n",
        "        #   #input_img = input_img * random_num\n",
        "        #   input_img = input_img * self.c\n",
        "        output_label = (input_label + 1) % 10\n",
        "        output_idx = 0\n",
        "        output_img = self.mnist_dataset[self.label_to_indices[output_label][output_idx]][0]\n",
        "\n",
        "        return input_img, output_img\n",
        "\n",
        "mnist_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
        "])\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n",
        "train_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
        "test_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = MNISTPairDataset(c,p,train_dataset)\n",
        "test_dataset = MNISTPairDataset(c,p,test_dataset)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Z5Ef7jfxwM"
      },
      "source": [
        "Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "leOlbBd8fxSC"
      },
      "outputs": [],
      "source": [
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "def calc_mean_std(feat, eps=1e-5):\n",
        "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
        "    size = feat.size()\n",
        "    assert (len(size) == 4)\n",
        "    N, C = size[:2]\n",
        "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "    return feat_mean, feat_std\n",
        "\n",
        "\n",
        "def adain(content_feat, style_feat):\n",
        "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
        "    size = content_feat.size()\n",
        "    style_mean, style_std = calc_mean_std(style_feat)\n",
        "    content_mean, content_std = calc_mean_std(content_feat)\n",
        "\n",
        "    normalized_feat = (content_feat - content_mean.expand(\n",
        "        size)) / content_std.expand(size)\n",
        "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
        "\n",
        "\n",
        "def _calc_feat_flatten_mean_std(feat):\n",
        "    # takes 3D feat (C, H, W), return mean and std of array within channels\n",
        "    assert (feat.size()[0] == 3)\n",
        "    assert (isinstance(feat, torch.FloatTensor))\n",
        "    feat_flatten = feat.view(3, -1)\n",
        "    mean = feat_flatten.mean(dim=-1, keepdim=True)\n",
        "    std = feat_flatten.std(dim=-1, keepdim=True)\n",
        "    return feat_flatten, mean, std\n",
        "\n",
        "\n",
        "def _mat_sqrt(x):\n",
        "    U, D, V = torch.svd(x)\n",
        "    return torch.mm(torch.mm(U, D.pow(0.5).diag()), V.t())\n",
        "\n",
        "\n",
        "def coral(source, target):\n",
        "    # assume both source and target are 3D array (C, H, W)\n",
        "    # Note: flatten -> f\n",
        "\n",
        "    source_f, source_f_mean, source_f_std = _calc_feat_flatten_mean_std(source)\n",
        "    source_f_norm = (source_f - source_f_mean.expand_as(\n",
        "        source_f)) / source_f_std.expand_as(source_f)\n",
        "    source_f_cov_eye = \\\n",
        "        torch.mm(source_f_norm, source_f_norm.t()) + torch.eye(3)\n",
        "\n",
        "    target_f, target_f_mean, target_f_std = _calc_feat_flatten_mean_std(target)\n",
        "    target_f_norm = (target_f - target_f_mean.expand_as(\n",
        "        target_f)) / target_f_std.expand_as(target_f)\n",
        "    target_f_cov_eye = \\\n",
        "        torch.mm(target_f_norm, target_f_norm.t()) + torch.eye(3)\n",
        "\n",
        "    source_f_norm_transfer = torch.mm(\n",
        "        _mat_sqrt(target_f_cov_eye),\n",
        "        torch.mm(torch.inverse(_mat_sqrt(source_f_cov_eye)),\n",
        "                 source_f_norm)\n",
        "    )\n",
        "\n",
        "    source_f_transfer = source_f_norm_transfer * \\\n",
        "                        target_f_std.expand_as(source_f_norm) + \\\n",
        "                        target_f_mean.expand_as(source_f_norm)\n",
        "\n",
        "    return source_f_transfer.view(source.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAvTE3D-fqX3"
      },
      "source": [
        "Net framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_-Xh2oSaSsMv"
      },
      "outputs": [],
      "source": [
        "Decoder = nn.Sequential(\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 3, (3, 3)),\n",
        ")\n",
        "\n",
        "vgg = nn.Sequential(\n",
        "    nn.Conv2d(3, 3, (1, 1)),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(3, 64, (3, 3)),\n",
        "    nn.ReLU(),  # relu1-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 64, (3, 3)),\n",
        "    nn.ReLU(),  # relu1-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(64, 128, (3, 3)),\n",
        "    nn.ReLU(),  # relu2-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 128, (3, 3)),\n",
        "    nn.ReLU(),  # relu2-2\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(128, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 256, (3, 3)),\n",
        "    nn.ReLU(),  # relu3-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(256, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-1, this is the last layer used\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu4-4\n",
        "    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-1\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-2\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU(),  # relu5-3\n",
        "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
        "    nn.Conv2d(512, 512, (3, 3)),\n",
        "    nn.ReLU()  # relu5-4\n",
        ")\n",
        "\n",
        "# class VQVAEcodebook(nn.Module):\n",
        "#     def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n",
        "#         super(VQVAEcodebook, self).__init__()\n",
        "\n",
        "#         self.vq_embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "#         self.vq_embedding.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings)\n",
        "#         # self.conv1 = nn.Conv2d(512,embedding_dim,1)\n",
        "#         # self.conv2 = nn.Conv2d(embedding_dim,512,1)\n",
        "\n",
        "#     def forward(self, ze):\n",
        "#         N, C, H, W = ze.shape\n",
        "#         embedding = self.vq_embedding.weight.data\n",
        "#         K, _ = embedding.shape\n",
        "\n",
        "#         embedding_broadcast = embedding.reshape(1, K, C, 1, 1)\n",
        "#         ze_broadcast = ze.reshape(N, 1, C, H, W)\n",
        "\n",
        "#         distance = torch.sum((embedding_broadcast - ze_broadcast) ** 2, 2)  # (N,K,H,W)\n",
        "#         nearest_neghbor = torch.argmin(distance, 1)  # (N,H,W)\n",
        "\n",
        "#         # zq (N, C, H, W) : (N, H, W, C) -> (N, C, H, W)\n",
        "#         zq = self.vq_embedding(nearest_neghbor).permute(0, 3, 1, 2)\n",
        "\n",
        "#         codebook_output = ze + (zq - ze).detach()\n",
        "\n",
        "#         return ze, zq, codebook_output\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.res_block = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(dim, dim, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(dim, dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.res_block(x)\n",
        "        return torch.relu(x)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, encoder, decoder, n_embedding, dim):\n",
        "        super(Net, self).__init__()\n",
        "        enc_layers = list(encoder.children())\n",
        "        self.enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n",
        "        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n",
        "        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n",
        "        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n",
        "        self.decoder = decoder\n",
        "        #self.codebook = codebook\n",
        "        self.mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "        # fix the encoder and decoder\n",
        "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4']:\n",
        "            for param in getattr(self, name).parameters():\n",
        "                param.requires_grad = False\n",
        "        for param in self.decoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        #codebook\n",
        "        self.vq_embedding = nn.Embedding(n_embedding, dim)\n",
        "        self.vq_embedding.weight.data.uniform_(-1.0 / n_embedding, 1.0/n_embedding)\n",
        "\n",
        "        self.encoderx = nn.Sequential(\n",
        "            nn.Conv2d(512, dim, 3, 1, 1),\n",
        "            nn.ReLU(),#relu_1\n",
        "            nn.Conv2d(dim, dim, 3, 1, 1),\n",
        "            nn.ReLU(),#relu_2\n",
        "            nn.Conv2d(dim, dim, 3, 1, 1),\n",
        "            ResidualBlock(dim),\n",
        "            ResidualBlock(dim)\n",
        "        )\n",
        "\n",
        "        self.decoderx = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, 3, 1, 1),\n",
        "            ResidualBlock(dim),\n",
        "            ResidualBlock(dim),\n",
        "            nn.ConvTranspose2d(dim, dim, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(dim, 512, 3, 1, 1)\n",
        "        )\n",
        "    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n",
        "    def encode_with_intermediate(self, input):\n",
        "        results = [input]\n",
        "        for i in range(4):\n",
        "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
        "            results.append(func(results[-1]))\n",
        "        return results[1:]\n",
        "\n",
        "    # extract relu4_1 from input image\n",
        "    def encode(self, input):\n",
        "        for i in range(4):\n",
        "            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n",
        "        return input\n",
        "\n",
        "    def calc_content_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        assert (target.requires_grad is False)\n",
        "        return self.mse_loss(input, target)\n",
        "\n",
        "    def calc_style_loss(self, input, target):\n",
        "        assert (input.size() == target.size())\n",
        "        assert (target.requires_grad is False)\n",
        "        input_mean, input_std = calc_mean_std(input)\n",
        "        target_mean, target_std = calc_mean_std(target)\n",
        "        return self.mse_loss(input_mean, target_mean) + \\\n",
        "               self.mse_loss(input_std, target_std)\n",
        "\n",
        "    def forward(self, content, style, label, alpha):\n",
        "\n",
        "        x = self.encode(content)\n",
        "        mean,std = calc_mean_std(x)\n",
        "        x = (x - mean) / std\n",
        "\n",
        "        ze = self.encoderx(x)\n",
        "        embedding = self.vq_embedding.weight.data\n",
        "        N, C, H, W = ze.shape\n",
        "        K, _ = embedding.shape\n",
        "\n",
        "        embedding_broadcast = embedding.reshape(1, K, C, 1, 1)\n",
        "        ze_broadcast = ze.reshape(N, 1, C, H, W)\n",
        "\n",
        "        distance = torch.sum((embedding_broadcast - ze_broadcast) ** 2, 2)  # (N,K,H,W)\n",
        "        nearest_neghbor = torch.argmin(distance, 1)  # (N,H,W)\n",
        "\n",
        "\n",
        "        # zq (N, C, H, W) : (N, H, W, C) -> (N, C, H, W)\n",
        "        zq = self.vq_embedding(nearest_neghbor).permute(0, 3, 1, 2)\n",
        "\n",
        "        t = ze + (zq - ze).detach()\n",
        "        t = self.decoderx(t)\n",
        "\n",
        "        mean_t,std_t = calc_mean_std(t)\n",
        "        gt = (t - mean_t) / std_t\n",
        "        gt = gt * std + mean\n",
        "        gt = alpha * gt + (1 - alpha) * t\n",
        "        gt = self.decoder(gt)\n",
        "        label = self.encode(label)\n",
        "        gt_feature = self.encode(gt)\n",
        "\n",
        "        loss_r = mse_loss(label,gt_feature)\n",
        "        mean_label,std_label = calc_mean_std(label)\n",
        "        label = (label - mean_label) / std_label\n",
        "\n",
        "        # mean_t,std_t = calc_mean_std(t)\n",
        "        # t = (t - mean_t) / std_t\n",
        "\n",
        "        loss_rec = mse_loss(label,t)\n",
        "        return ze, zq, loss_rec, loss_r, gt\n",
        "##10_22 checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbI2hyCX0lIg"
      },
      "source": [
        "Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxiuCJ8TBU2W",
        "outputId": "d53a704f-3e0c-48f1-fd73-86e2fd0d329e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-e805ffc750d0>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"/content/final_model4_200_100.pth\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (enc_1): Sequential(\n",
              "    (0): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (2): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "  )\n",
              "  (enc_2): Sequential(\n",
              "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (4): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (6): ReLU()\n",
              "  )\n",
              "  (enc_3): Sequential(\n",
              "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (4): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (6): ReLU()\n",
              "  )\n",
              "  (enc_4): Sequential(\n",
              "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (2): ReLU()\n",
              "    (3): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "    (6): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=True)\n",
              "    (10): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (12): ReLU()\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (2): ReLU()\n",
              "    (3): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (4): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (9): ReLU()\n",
              "    (10): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (12): ReLU()\n",
              "    (13): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (14): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (15): ReLU()\n",
              "    (16): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (17): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (18): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (19): ReLU()\n",
              "    (20): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (21): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (22): ReLU()\n",
              "    (23): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (24): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (25): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (26): ReLU()\n",
              "    (27): ReflectionPad2d((1, 1, 1, 1))\n",
              "    (28): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1))\n",
              "  )\n",
              "  (mse_loss): MSELoss()\n",
              "  (vq_embedding): Embedding(200, 100)\n",
              "  (encoderx): Sequential(\n",
              "    (0): Conv2d(512, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): ResidualBlock(\n",
              "      (res_block): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(100, 100, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (6): ResidualBlock(\n",
              "      (res_block): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(100, 100, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoderx): Sequential(\n",
              "    (0): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ResidualBlock(\n",
              "      (res_block): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(100, 100, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (res_block): Sequential(\n",
              "        (0): ReLU()\n",
              "        (1): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(100, 100, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (3): ConvTranspose2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): ConvTranspose2d(100, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "decoder = Decoder\n",
        "encoder = vgg\n",
        "model = Net(encoder, decoder, 200,100)\n",
        "state_dict = torch.load(\"/content/final_model4_200_100.pth\")\n",
        "model.load_state_dict(state_dict)\n",
        "model.train()\n",
        "model.to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hGi6EnSgMwr"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "oUDd9M2-fv-t",
        "outputId": "906a4827-6cde-43ea-9ede-ca83d4d2bc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start vqvae train...\n",
            "epoch:0, loss_overall:1553.172119\n",
            "epoch:0, loss_rec:  844.832581\n",
            "epoch:0, loss_zq:   566.672424\n",
            "\n",
            "epoch:1, loss_overall:1523.729370\n",
            "epoch:1, loss_rec:  831.809143\n",
            "epoch:1, loss_zq:   553.537354\n",
            "\n",
            "epoch:2, loss_overall:1522.709595\n",
            "epoch:2, loss_rec:  829.095764\n",
            "epoch:2, loss_zq:   554.890259\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-7dbbaf15111c>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moverall_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moverall_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# N1HW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#optimizer = torch.optim.Adam(model.parameters(),lr=1e-4,weight_decay=1e-4)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 1e-4 ,momentum=0.9)\n",
        "epochs = 5\n",
        "alpha=1\n",
        "beta= 1\n",
        "gamma = 0.25\n",
        "minloss = 1e9\n",
        "checkpoint = \"vqvae_checkpoints.pth\"\n",
        "print(\"start vqvae train...\")\n",
        "\n",
        "losslist = []\n",
        "for epo in range(epochs):\n",
        "    overall_loss = 0\n",
        "    overall_zq = 0\n",
        "    overall_z = 0\n",
        "    overall_r = 0\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        x = x.to(DEVICE)  # N1HW\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        ze, zq, loss_rec, loss_r ,x_hat = model(x,x,y,1)\n",
        "        loss_ze = mse_loss(zq.detach(), ze)\n",
        "        loss_zq = mse_loss(zq, ze.detach())\n",
        "\n",
        "        loss = loss_zq + 0.25 * loss_ze + loss_rec + loss_r\n",
        "\n",
        "        overall_loss += loss\n",
        "        overall_r += loss_rec + loss_r\n",
        "        overall_zq += loss_zq\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #break\n",
        "    losslist.append(overall_loss.item())\n",
        "    print(f\"epoch:{epo}, loss_overall:{overall_loss.item():.6f}\")\n",
        "    print(f\"epoch:{epo}, loss_rec:  {overall_r.item():.6f}\")\n",
        "    print(f\"epoch:{epo}, loss_zq:   {overall_zq.item():.6f}\")\n",
        "    #print(f\"epoch:{epo}, loss_z:   {overall_z.item():.6f}\")\n",
        "    print()\n",
        "    #break\n",
        "    # if (epo+1) % 10 == 0:\n",
        "    #     torch.save(model.state_dict(), checkpoint)\n",
        "    if minloss > overall_loss:\n",
        "        torch.save(model.state_dict(), checkpoint)\n",
        "        minloss = overall_loss\n",
        "print(\"vqvae train finish!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTuTIxnTClvk"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cy1FljqsnPT",
        "outputId": "49376f82-c5da-4774-c76a-8fadaa93ea04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-76585b797bf3>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\"/content/model4_200_100.pth\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_model = Net(encoder, decoder, 200, 100)\n",
        "state_dict = torch.load(\"/content/model4_200_100.pth\")\n",
        "test_model.load_state_dict(state_dict)\n",
        "test_model.to(DEVICE)\n",
        "test_model.eval()\n",
        "import cv2\n",
        "import einops\n",
        "def reconstruct(model, x, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _, _,_,_, x_hat = model(x,x,x,0.9)\n",
        "    print(x_hat.shape)\n",
        "    n = x.shape[0]\n",
        "    n1 = 5\n",
        "\n",
        "    x_cat = torch.concat((x, x_hat), 3)\n",
        "    x_cat = einops.rearrange(x_cat, '(n1 n2) c h w -> (n1 h) (n2 w) c', n1=n1)\n",
        "    x_cat = (x_cat.clip(0, 1) * 255).cpu().numpy().astype(np.uint8)\n",
        "    cv2.imwrite(f'reconstruct_show.jpg', x_cat)\n",
        "\n",
        "batch_imgs, _ = next(iter(test_loader))\n",
        "batch_imgs = batch_imgs.to(DEVICE)\n",
        "reconstruct(test_model, batch_imgs, DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTDue8KR26Xe",
        "outputId": "a16c9a67-e261-42cf-d97c-86d0b6c40dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "import einops\n",
        "def reconstruct(model, x, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _,_,_,_,x_hat = model(x,x,x,1)\n",
        "    print(x_hat.shape)\n",
        "    n = x.shape[0]\n",
        "    n1 = 5\n",
        "\n",
        "    x_cat = torch.concat((x, x_hat), 3)\n",
        "    x_cat = einops.rearrange(x_cat, '(n1 n2) c h w -> (n1 h) (n2 w) c', n1=n1)\n",
        "    x_cat = (x_cat.clip(0, 1) * 255).cpu().numpy().astype(np.uint8)\n",
        "    cv2.imwrite(f'reconstruct_show.jpg', x_cat)\n",
        "\n",
        "batch_imgs, _ = next(iter(test_loader))\n",
        "batch_imgs = batch_imgs.to(DEVICE)\n",
        "reconstruct(model, batch_imgs, DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMp6BkL8F2ok"
      },
      "source": [
        "Pre with VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNCPn-b2F5V6"
      },
      "outputs": [],
      "source": [
        "def style_transfer(vgg, decoder, content, style, alpha=1.0,\n",
        "                   interpolation_weights=None):\n",
        "    assert (0.0 <= alpha <= 1.0)\n",
        "    content_f = vgg(content)\n",
        "    style_f = vgg(style)\n",
        "    feat = adain(content_f, style_f)\n",
        "    feat = feat * alpha + content_f * (1 - alpha)\n",
        "    # mean,std = calc_mean_std(content_f)\n",
        "    # feat = (content_f - mean) / std\n",
        "    return decoder(feat)\n",
        "import cv2\n",
        "import einops\n",
        "def vgg_reconstruct(x, device):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_hat = style_transfer(encoder,decoder,x,x)\n",
        "\n",
        "    n = x.shape[0]\n",
        "    n1 = 1\n",
        "    x_cat = torch.concat((x, x_hat), 3)\n",
        "    x_cat = einops.rearrange(x_cat, '(n1 n2) c h w -> (n1 h) (n2 w) c', n1=n1)\n",
        "    x_cat = (x_cat.clip(0, 1) * 255).cpu().numpy().astype(np.uint8)\n",
        "    cv2.imwrite(f'reconstruct_show.jpg', x_cat)\n",
        "\n",
        "\n",
        "# batch_imgs, _ = next(iter(test_loader))\n",
        "# batch_imgs = batch_imgs.to(DEVICE)\n",
        "# vgg_reconstruct(batch_imgs, DEVICE)\n",
        "image_path = '/content/2_172.png'  # 替换为你的图片路径\n",
        "image = Image.open(image_path)\n",
        "# image = image.convert('L')\n",
        "# image = ImageOps.invert(image)\n",
        "# threshold = 20\n",
        "# image = image.point(lambda p: 255 if p > threshold else 0)\n",
        "image_preprocess = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),  # 转换为张量，且归一化到 [0, 1]\n",
        "    transforms.Lambda(lambda x: x[[2, 1, 0], :, :])\n",
        "    #transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
        "])\n",
        "input_tensor = image_preprocess(image)  # 得到预处理后的张量\n",
        "input_batch = input_tensor.unsqueeze(0)  # 增加batch维度，变为(1, C, H, W) 适应模型输入\n",
        "input_batch = input_batch.to(DEVICE)\n",
        "vgg_reconstruct(input_batch, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG7Xvc5zHV7o",
        "outputId": "1c97ae64-9737-4a95-b637-83b446118760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "def reconstruct(model, x, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _,_,_,_,x_hat = model(x,x,x,1)\n",
        "    print(x_hat.shape)\n",
        "    n = x.shape[0]\n",
        "    n1 = 1\n",
        "\n",
        "    x_cat = torch.concat((x, x_hat), 3)\n",
        "    x_cat = einops.rearrange(x_cat, '(n1 n2) c h w -> (n1 h) (n2 w) c', n1=n1)\n",
        "    x_cat = (x_cat.clip(0, 1) * 255).cpu().numpy().astype(np.uint8)\n",
        "    cv2.imwrite(f'reconstruct_show.jpg', x_cat)\n",
        "\n",
        "# test_model = VQVAE(1,200,10)\n",
        "# state_dict = torch.load(\"/content/vqvae_checkpoints_2024_10_9_epoch5_loss5.pth\")\n",
        "# test_model.load_state_dict(state_dict)\n",
        "# test_model.to(DEVICE)\n",
        "# test_model.eval()\n",
        "image_path = '/content/0_44.png'  # 替换为你的图片路径\n",
        "image = Image.open(image_path)\n",
        "# image = image.convert('L')\n",
        "# image = ImageOps.invert(image)\n",
        "# threshold = 20\n",
        "# image = image.point(lambda p: 255 if p > threshold else 0)\n",
        "image_preprocess = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),  # 转换为张量，且归一化到 [0, 1]\n",
        "    transforms.Lambda(lambda x: x[[2, 1, 0], :, :])\n",
        "    #transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
        "])\n",
        "input_tensor = image_preprocess(image)  # 得到预处理后的张量\n",
        "input_batch = input_tensor.unsqueeze(0)  # 增加batch维度，变为(1, C, H, W) 适应模型输入\n",
        "input_batch = input_batch.to(DEVICE)\n",
        "reconstruct(model, input_batch, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40JzOssflhMC"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6hrVmIR6afx",
        "outputId": "67c67ab8-627b-4d5f-ef86-2f3fce913927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1205\n"
          ]
        }
      ],
      "source": [
        "print(minloss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VSKIUo4mACKq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cv2\n",
        "import einops\n",
        "def reconstruct(x, y, device):\n",
        "    # model.to(device)\n",
        "    # model.eval()\n",
        "    # with torch.no_grad():\n",
        "    #     _, _,_, _,x_hat = model(x,x,x,0.9)\n",
        "    #print(x_hat.shape)\n",
        "    n = x.shape[0]\n",
        "    n1 = 5\n",
        "\n",
        "    x_cat = torch.concat((x, y), 3)\n",
        "    x_cat = einops.rearrange(x_cat, '(n1 n2) c h w -> (n1 h) (n2 w) c', n1=n1)\n",
        "    x_cat = (x_cat.clip(0, 1) * 255).cpu().numpy().astype(np.uint8)\n",
        "    cv2.imwrite(f'reconstruct_show.jpg', x_cat)\n",
        "\n",
        "batch_imgs, _ = next(iter(test_loader))\n",
        "batch_imgs = batch_imgs.to(DEVICE)\n",
        "_ = _.to(DEVICE)\n",
        "reconstruct(batch_imgs,_, DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cedV_1LsANhz",
        "outputId": "1a8d2aef-e313-4d9d-c275-3a87a0ca384c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2546.0906, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(minloss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hGxEBaF1NnW6"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import einops\n",
        "def reconstruct(model, x,y, device,path):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        alpha = 1\n",
        "        _, _, _,_,x_hat = model(x,x,x,alpha)\n",
        "\n",
        "    n = x.shape[0]\n",
        "    n1 = 1\n",
        "    x_cat = torch.concat((torch.concat((x, x_hat), 2),y),2)\n",
        "    x_cat = einops.rearrange(x_cat, '(n1 n2) c h w -> (n1 h) (n2 w) c', n1=n1)\n",
        "    x_cat = (x_cat.clip(0, 1) * 255).cpu().numpy().astype(np.uint8)\n",
        "    cv2.imwrite(path, x_cat)\n",
        "\n",
        "model.eval()\n",
        "batch_imgs, _ = next(iter(test_loader))\n",
        "batch_img = batch_imgs[10].unsqueeze(0)\n",
        "label_img = _[10].unsqueeze(0)\n",
        "batch_imgs = batch_imgs.to(DEVICE)\n",
        "label_img = label_img.to(DEVICE)\n",
        "batch_img = batch_img.to(DEVICE)\n",
        "reconstruct(model, batch_img,label_img, DEVICE,'reconstruct_show.jpg')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = VQVAE(3,300,10)\n",
        "# state_dict = torch.load(\"/content/vqvae_checkpoints_10_22.pth\")\n",
        "# model.load_state_dict(state_dict)\n",
        "# model.to(DEVICE)\n",
        "# model.eval()\n",
        "import cv2\n",
        "import einops\n",
        "alpha = 1.0\n",
        "def reconstruct(model, x, y,device,path):\n",
        "    # model.to(device)\n",
        "    # model.eval()\n",
        "    # with torch.no_grad():\n",
        "    #     _, _, _,x_hat = model(x,x,x,alpha)\n",
        "\n",
        "    # n = x.shape[0]\n",
        "    # n1 = int(n**0.5)\n",
        "    # x_cat = torch.concat((x, x_hat), 3)\n",
        "    # x_cat = einops.rearrange(x_cat, '(n1 n2) c h w -> (n1 h) (n2 w) c', n1=n1)\n",
        "    # x_cat = (x_cat.clip(0, 1) * 255).cpu().numpy().astype(np.uint8)\n",
        "    # cv2.imwrite(path, x_cat)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        alpha = 1\n",
        "        _, _, _,_,x_hat = model(x,x,x,alpha)\n",
        "\n",
        "    n = x.shape[0]\n",
        "    n1 = 1\n",
        "    x_cat = torch.concat((torch.concat((x, x_hat), 2),y),2)\n",
        "    x_cat = einops.rearrange(x_cat, '(n1 n2) c h w -> (n1 h) (n2 w) c', n1=n1)\n",
        "    x_cat = (x_cat.clip(0, 1) * 255).cpu().numpy().astype(np.uint8)\n",
        "    cv2.imwrite(path, x_cat)\n",
        "image_path = '/content/8_45.png'  # 替换为你的图片路径\n",
        "out_put_path = '/content/9_45.png'\n",
        "image = Image.open(image_path)\n",
        "# image = image.convert('L')\n",
        "# image = ImageOps.invert(image)\n",
        "# threshold = 20\n",
        "# image = image.point(lambda p: 255 if p > threshold else 0)\n",
        "image_preprocess = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),  # 转换为张量，且归一化到 [0, 1]\n",
        "    transforms.Lambda(lambda x: x[[2, 1, 0], :, :])\n",
        "    #transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
        "])\n",
        "input_tensor = image_preprocess(image)  # 得到预处理后的张量\n",
        "input_batch = input_tensor.unsqueeze(0)  # 增加batch维度，变为(1, C, H, W) 适应模型输入\n",
        "input_batch = input_batch.to(DEVICE)\n",
        "image = Image.open(out_put_path)\n",
        "out_tensor = image_preprocess(image)\n",
        "out_batch = out_tensor.unsqueeze(0)\n",
        "out_batch = out_batch.to(DEVICE)\n",
        "reconstruct(model, input_batch, out_batch,DEVICE,'test.png')"
      ],
      "metadata": {
        "id": "IPbVwdQjk5av"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-X7sRbOlasX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}